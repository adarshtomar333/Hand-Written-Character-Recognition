{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtauA0Vfu7HDuv83RnR7W2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adarshtomar333/Hand-Written-Character-Recognition/blob/main/hcr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhe7LOPyabIR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR']  = '/content'\n",
        "!kaggle datasets download -d vaibhao/handwritten-characters\n",
        "!unzip \\*.zip && rm *.zip"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9OmZlNmiJ5W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify if the directories exist and print some sample files\n",
        "train_image_dir = '/content/Train'\n",
        "test_image_dir = '/content/Validation'\n",
        "\n",
        "if not os.path.exists(train_image_dir) or not os.path.exists(test_image_dir):\n",
        "    raise Exception(\"Image directories not found. Please check the paths.\")\n",
        "\n",
        "# print(\"Sample training images:\", os.listdir(train_image_dir)[:5])\n",
        "# print(\"Sample testing images:\", os.listdir(test_image_dir)[:5])"
      ],
      "metadata": {
        "id": "x6KQwuomKDnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the paths to the directories\n",
        "train_dir = '/content/dataset/Train'\n",
        "test_dir = '/content/dataset/Validation'\n",
        "\n",
        "# Load and preprocess the training dataset\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    color_mode='grayscale',\n",
        "    batch_size=32,\n",
        "    image_size=(28, 28),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load and preprocess the testing dataset\n",
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    color_mode='grayscale',\n",
        "    batch_size=32,\n",
        "    image_size=(28, 28),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Extract class names before mapping\n",
        "class_names = train_dataset.class_names\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Normalize the images to the range [0, 1]\n",
        "def normalize_img(image, label):\n",
        "    return tf.cast(image, tf.float32) / 255.0, label\n",
        "\n",
        "train_dataset = train_dataset.map(normalize_img)\n",
        "test_dataset = test_dataset.map(normalize_img)\n",
        "\n",
        "# Convert datasets to use in the model\n",
        "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Build the CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9q2d2s01ZKby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_dataset, verbose=2)\n",
        "print(f'Test accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "id": "QU9R3xbllRR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize training results\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ULUIThM5nrB3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to an .h5 file\n",
        "model.save('handwritten_character_recognition_model.h5')\n",
        "\n",
        "# Save the model architecture to a JSON file\n",
        "model_json = model.to_json()\n",
        "with open(\"model_architecture.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Save the model weights to an HDF5 file\n",
        "model.save_weights(\"model_weights.h5\")"
      ],
      "metadata": {
        "id": "GScalfq_nyvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the files module for downloading files\n",
        "from google.colab import files\n",
        "\n",
        "# Download the .h5 file\n",
        "files.download('handwritten_character_recognition_model.h5')\n",
        "\n",
        "# Download the JSON file\n",
        "files.download('model_architecture.json')\n",
        "\n",
        "# Download the weights file\n",
        "files.download('model_weights.h5')"
      ],
      "metadata": {
        "id": "wmnQiEMcMOI5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}